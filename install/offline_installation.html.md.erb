---
title: Installing Ops Manager in Air-Gapped Environments
owner: Ops Manager
---

This topic provides an overview of the components and resources needed to install <%= vars.ops_manager_first %> in air-gapped environments, including the typical corresponding automation resources.


## <a id='offline-components'></a> Offline Components

To run offline, you must obtain resources from the internet and move them into offline components that store and use them.
The method you use to move a resource from VMware Tanzu Network or GitHub
into an offline environment can vary from setting up a designated proxy to burning a DVD.

The following image displays types of resources and the components you must move them to in your offline environment. It also includes a jumpbox. The jumpbox is a Linux host for running commands such as `bosh`, `uaac`, and `fly`. You could use the <%= vars.ops_manager %> VM for this purpose. Blocks shown in this configuration are:

  * Ops Manager Network Resources, feeding into:
    * S3
    * Artifact Store
  * Automation Resources, feeding into:
    * Artifact Store
    * Docker Registry
    * Git
    * Runway /Concourse

![alt-text=""](images/offline-components.png)

[//]: (https://docs.google.com/drawings/d/1FB7ppD2MJna8LOiyU1OhLmHxlqcXamkkeiR4P6D5d6U/edit?usp=sharing)

The following table provides more detail about resources and the component you must move them to:

<table>
  <tr>
    <th>Resource</th>
    <th>Component</th>
  </tr>
  <tr>
    <td>VMware Tanzu Network products such as tiles, stemcells, BOSH releases, and <%= vars.ops_manager %></td>
    <td>S3 and artifact store</td>
  </tr>
  <tr>
    <td>Pipelines and scripts</td>
    <td>Git</td>
  </tr>
  <tr>
    <td>Configuration</td>
    <td>Git</td>
  </tr>
  <tr>
    <td>Container images</td>
    <td>Docker Registry, S3, or artifact store</td>
  </tr>
  <tr>
    <td>Third-party resources such as NSX-T and OSS BOSH releases</td>
    <td>Artifact store</td>
  </tr>
  <tr>
    <td>Backup artifacts</td>
    <td>S3</td>
  </tr>
</table>


## <a id='architectural-patterns'></a> Architectural Patterns

The following sections describe three architectural patterns for running in air-gapped environments.
The pattern you use depends on how your environment is set up.

### <a id='pattern-one'></a> Pattern One: Artifact Store with Internet Access

It is common for organizations to deploy an artifact store such as Artifactory or Nexus  as a gateway to the Internet. In this configuration, the artifact store is typically configured as a Docker mirror.

This pattern includes an S3 component that is used only for backups of the installation.
Blocks in this configuration are:

  * Jumpbox
  * S3
  * Git
  * Runway / Concourse, feeing into:
    * Ops Manager Network Resources
    * Artifact Store
  * Automation Resources, feeding into:
    * Artifact Store

![alt-text=""](images/internet-artifact-store.png)

[//]: (https://docs.google.com/drawings/d/1T-nfzsXZCtQL7vcQhso8AGun7CZGFdkO7yQ309a_l8k/edit?usp=sharing)

The following table provides an example of how you might handle resources in this scenario:

<table>
  <tr>
    <th>Resource</th>
    <th>Component</th>
  </tr>
  <tr>
    <td>VMware Tanzu Network products such as tiles, stemcells, BOSH releases, and <%= vars.ops_manager %></td>
    <td>This architectural pattern presents the challenge of getting VMware Tanzu Network resources from the Internet into the artifact store. For example, Artifactory and Nexus cannot interact directly with VMware Tanzu Network. You could place VMware Tanzu Network resources in an artifact store either through a remote Concourse worker with access to the Internet, by downloading them from the Internet and placing them in the artifact store manually, or some other method.</td>
  </tr>
  <tr>
    <td>Pipelines and scripts</td>
    <td>Store in Git. Use manual clone and push. Modify pipelines to use <code>registry_mirror</code>.</td>
  </tr>
  <tr>
    <td>Configuration</td>
    <td>Store in Git. Use manual clone and push.</td>
  </tr>
  <tr>
    <td>Container images</td>
    <td>Store in the artifact store that you configured as a mirror.</td>
  </tr>
  <tr>
    <td>Third-party resources such as NSX-T and OSS BOSH releases</td>
    <td>Store in a generic artifact store repository.</td>
  </tr>
  <tr>
    <td>Backup artifacts</td>
    <td>Store in a S3 blobstore.</td>
  </tr>
</table>


### <a id='pattern-two'></a> Pattern Two: Completely Offline

In completely offline environments, you must bring in all resources manually and store them in the available local components.
Blocks shown in this configuration are:

  * Jumpbox
  * Ops Manager Network Resources, feeding into:
    * Operator and media
  * Automation Resources, feeding into:
    * Operator and media
  * Operator and media, feeding into:
    * S3
    * Artifact Store
    * Docker Registry
    * Git
    * Runway / Concourse

![alt-text=""](images/completely-offline.png)

[//]: (https://docs.google.com/drawings/d/1eo0rE2n1j1OqoHgwvKrHvyPKBF1PgKPdh8Dfs1hQgpk/edit?usp=sharing)

In this architectural pattern, you must configure pipelines to watch components for changes and apply updates when available. The following table provides an example of how you might handle resources in this scenario:

<table>
  <tr>
    <th>Resource</th>
    <th>Component</th>
  </tr>
  <tr>
    <td>VMware Tanzu Network products such as tiles, stemcells, BOSH releases, and <%= vars.ops_manager %></td>
    <td>Manually upload to Nexus.</td>
  </tr>
  <tr>
    <td>Pipelines and scripts</td>
    <td>Modify pipelines to use a local Harbor Docker registry. Manually clone an online environment, bring it to the offline environment on DVD, and push it to offline Git.</td>
  </tr>
  <tr>
    <td>Configuration</td>
    <td>Store in your offline Git.</td>
  </tr>
  <tr>
    <td>Container Images</td>
    <td>Get from the Internet, transfer to USB, and push to offline Harbor.</td>
  </tr>
  <tr>
    <td>Third-party resources such as NSX-T and OSS BOSH releases</td>
    <td>Store in a generic artifact store repository.</td>
  </tr>
  <tr>
    <td>Backup artifacts</td>
    <td>Store in a S3 blobstore.</td>
  </tr>
</table>

### <a id='pattern-three'></a> Pattern Three: TLS Intercepting Proxy

This pattern allows Internet access, but only through a proxy that decrypts and rewrites  TLS certificates. In general, this resembles an online install, however it requires that you add your corporate certificate to all BOSH-deployed VMs.
Blocks shown in this configuration are:

  * Jumpbox
  * Ops Manager Network Resources block and Automation Resources block, feed (through a proxy) into:
    * S3
    * Artifact Store
    * Docker Registry
    * Git
    * Runway / Concourse

![alt-text=""](images/tls-intercepting-proxy.png)

[//]: (https://docs.google.com/drawings/d/1kq55um8ZBnYbvCrJE0dpuo3T8iM2MoKNhdDEShabNM8/edit?usp=sharing)

Some of the challenges in this pattern include the following:

* Any automation that calls to the Internet will fail if it does not have the corporate certificate in its trust store.
* Concourse tasks that do not use resources do not receive updated BOSH root certificates. This means you must configure tasks to ignore TLS errors or update the root certificates as part of the tasks.
