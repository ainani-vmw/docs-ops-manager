---
title: OpenStack reference architecture for Tanzu Operations Manager
owner: Customer0
---

This topic tells you about the OpenStack reference architecture for <%= vars.app_runtime_first %>, which runs on <%= vars.ops_manager_first %>. This architecture is
valid for most production grade <%= vars.ops_manager %> deployments in a single project using three availability zones (AZs).

For general requirements for running <%= vars.ops_manager %> and specific requirements for running <%= vars.ops_manager %> on OpenStack, see [OpenStack on <%= vars.ops_manager %> requirements](../../install/openstack.html).

## <a id='refarchs'></a><%= vars.ops_manager %> reference architectures

An <%= vars.ops_manager %> reference architecture describes a proven approach for deploying <%= vars.ops_manager %> on a specific IaaS, such as OpenStack, that meets these requirements:

* Secure

* Publicly-accessible

* Includes common <%= vars.ops_manager %>-managed runtimes and services, such as  for <%= vars.app_runtime_abbr %>, VMware Tanzu SQL, VMware Tanzu RabbitMQ, and Spring Cloud Services for VMware Tanzu

* Can host at least 100 app instances

<%= vars.company_name %> provides reference architectures to help you determine the best configuration for your <%= vars.ops_manager %> deployment.

## <a id='base_components'></a> Base reference architecture components

The following table lists the components that are part of a base reference architecture deployment on OpenStack with three AZs:

<table class=“table”>
<thead>
<th><strong>Component</strong></th>
<th><strong>Reference architecture notes</strong></th>
 </thead>
<tr>
    <td><strong>Domains and DNS</strong></td>
    <td>Domain zones and routes in use by the reference architecture include:<br><br>
      <ul>
         <li>Zones for *.apps and *.sys (required)</li>
         <li>A route for <%= vars.ops_manager %> (required)</li>
         <li>A route for Doppler (required)</li>
         <li>A route for Loggregator (required)</li>
         <li>A route for SSH access to app containers (optional)</li>
         <li>A route for TCP access to TCP routers (optional)</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td><strong><%= vars.ops_manager %> VM</strong></td>
    <td>Deployed on the infrastructure network and accessible by fully-qualified domain name (FQDN) or through an optional jumpbox.</td>
  </tr>
  <tr>
    <td><strong>BOSH Director</strong></td>
    <td>Deployed on the infrastructure network.</td>
  </tr>
  <tr>
    <td><strong>Application Load Balancer</strong></td>
    <td>Required. Load balancer that handles incoming HTTP, HTTPS, TCP, and SSL traffic and forwards them to the GoRouters. Load balancers are outside the scope of this topic.</td>
  </tr>
  <tr>
    <td><strong>SSH Load Balancer</strong></td>
    <td>Optional. Load balancer that provides SSH access to app containers for developers. Load balancers are outside the scope of this topic.</td>
  </tr>
  <tr>
    <td><strong>GoRouters</strong></td>
    <td>Accessed through the Application Load Balancer. Deployed on the <%= vars.app_runtime_abbr %> network, one per AZ.</td>
  </tr>
  <tr>
    <td><strong>Diego Brains</strong></td>
    <td>This component is required. However, the SSH container access functionality is optional and enabled through the SSH load balancers. Deployed on the <%= vars.app_runtime_abbr %> network, one per AZ.</td>
  </tr>
  <tr>
    <td><strong>TCP Routers</strong></td>
    <td>Optional feature for TCP routing. Deployed on the <%= vars.app_runtime_abbr %> network, one per AZ.</td>
  </tr>
  <tr>
    <td><strong>Database</strong></td>
    <td>Reference architecture uses internal MySQL.</td>
  </tr>
  <tr>
    <td><strong>Storage Buckets</strong></td>
    <td>Reference architecture uses customer-provided blobstore. Buckets are needed for BOSH and <%= vars.app_runtime_abbr %>.</td>
  </tr>
  <tr>
    <td><strong>Service Tiles</strong></td>
    <td>Deployed on the services network.</td>
  </tr>
  <tr>
  	<td><strong>Service Accounts</strong></td>
  	<td><%= vars.company_name %> recommends two service accounts: one for OpenStack "paving," and the other for <%= vars.ops_manager %> and BOSH.<br><br>
      <ul>
        <li><strong>Admin Account:</strong> Concourse uses this account to provision required OpenStack resources as well as a Keystone service account.</li>
  		  <li><strong>Keystone Service Account:</strong> This service account is automatically provisioned with restricted access only to resources needed by <%= vars.ops_manager %> and BOSH.
      </ul>
  	 </td>
  </tr>
  <tr>
  	 <td><strong>OpenStack Quota</strong></td>
  	 <td>The default compute quota on a new OpenStack subscription is typically not enough to host a multi-AZ deployment. <%= vars.company_name %> recommends a quota of 100 for instances. Your OpenStack network quotas may also need to be increased.</td>
  </tr>
</table>

## <a id='openstack_components'></a> OpenStack objects

The following table lists the network objects in this reference architecture:
<table class=“table”>
<thead>
<th><strong>Network Object</strong></th>
<th><strong>Notes</strong></th>
<th><strong>Estimated Number</strong></th>
  <tr>
  </thead>
    <td><strong>Floating IP addresses</strong></td>
    <td>Two per deployment: one assigned to <%= vars.ops_manager %>, the other to your jumpbox.</td>
    <td>2</td>
  </tr>
  </thead>
  <tr>
    <td><strong>Project</strong></td>
    <td>One per deployment. A deployment exists within a single project and a single OpenStack region, but should distribute <%= vars.app_runtime_abbr %> jobs and service instances across three OpenStack AZs to ensure high availability.</td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>Networks</strong></td>
    <td>The reference architecture requires these Tenant Networks:
      <ul>
        <li>1 x (/24) <strong>Infrastructure</strong> (<%= vars.ops_manager %>, BOSH Director, Jumpbox).</li>
        <li>1 x (/20) <strong><%= vars.app_runtime_abbr %></strong> (GoRouters, Diego Cells, Cloud Controllers, etc.).</li>
        <li>1 x (/20) <strong>Services</strong> (VMware Tanzu SQL, VMware Tanzu RabbitMQ, and Spring Cloud Services for VMware Tanzu, etc.)</li>
        <li>1 x (/24) <strong>On-demand services</strong> (various)</li>
      </ul>
      An Internet-facing network is also required:
      <ul>
        <li>One <strong>Public</strong> network.<br/>
      </ul>
      <p> In many cases, the public network is an "under the cloud" network that is shared across projects.</p>
    </td>
    <td>5</td>
  </tr>
  <tr>
    <td><strong>Routers</strong></td>
    <td>This reference architecture requires one router attached to all networks:
    	<ul>
    	   <li><strong>VirtualRouter:</strong> This router table enables the ingress and egress routes to and from the Internet to the project networks and provides SNAT services.</li>
    	</ul>
    </td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>Security Groups</strong></td>
    <td>The reference architecture requires one Security Groups. The following table describes the Security Group ingress rules:
    <table class=“table”>
      <thead>
    	<tr>
    		<th>Security Group</th>
    		<th>Port</th>
    		<th>From CIDR</th>
    		<th>Protocol</th>
    		<th>Description</th>
    	</tr>
      </thead>
    	<tr>
    		<td>OpsMgrSG</td>
    		<td>22</td>
    		<td>0.0.0.0/0</td>
    		<td>TCP</td>
    		<td><%= vars.ops_manager %> VM SSH access</td>
    	</tr>
    	<tr>
    		<td>OpsMgrSG</td>
    		<td>443</td>
    		<td>0.0.0.0/0</td>
    		<td>TCP</td>
    		<td><%= vars.ops_manager %> VM HTTP access</td>
    	</tr>
    	<tr>
    		<td>VmsSG</td>
    		<td>ALL</td>
    		<td>VPC_CIDR</td>
    		<td>ALL</td>
    		<td>Open up connections among BOSH-deployed VMs</td>
    	</tr>
    </table>
    Additional security groups may be needed which are specific to your chosen load balancing solution.
    </td>
    <td>5</td>
  </tr>
  <tr>
    <td><strong>Load Balancers</strong></td>
    <td><%= vars.ops_manager %> on OpenStack requires a load balancer, which can be configured with multiple listeners to forward HTTP, HTTPS, and TCP traffic. <%= vars.company_name %> recommends two load balancers: one to forward the traffic to the GoRouters, <code>AppsLB</code>; and the other to forward the traffic to the Diego Brain SSH proxy, <code>SSHLB</code>.
    <br><br>
    The following table describes the required listeners for each load balancer:
    <table class=“table”>
    <thead>
    		<tr>
    			<th>Name</th>
    			<th>Instance/Port</th>
    			<th>Load Balancer Port</th>
    			<th>Protocol</th>
    			<th>Description</th>
    		</tr>
        </thead>
    		<tr>
    			<td>AppsLB</td>
    			<td>gorouter/80</td>
    			<td>80</td>
    			<td>HTTP</td>
    			<td>Forward traffic to GoRouters
    		</tr>
    		<tr>
    			<td>AppsLB</td>
    			<td>gorouter/80</td>
    			<td>443</td>
    			<td>HTTPS</td>
    			<td>SSL termination and forward traffic to GoRouters</td>
    		</tr>
    		<tr>
    			<td>SSHLB</td>
    			<td>diego-brain/2222</td>
    			<td>2222</td>
    			<td>TCP</td>
    			<td>Forward traffic to Diego Brain for container SSH connections</td>
    		</tr>
    	</table>
    Each load balancer needs a check to validate the health of the back end instances:
      <ul>
    	   <li><code>AppsLB</code> checks the health on GoRouter port 80 with TCP.</li>
    	   <li><code>SSHLB</code> checks the health on Diego Brain port 2222 with TCP.</li>
      </ul>
      <p> In many cases, the load balancers are provided as an "under the cloud" service that is shared across projects.</p>
    </td>
    <td>2</td>
  </tr>
  <tr>
    <td><strong>Jumpbox</strong></td>
    <td>Optional. Provides a way of accessing different network components. For example, you can configure it with your own permissions and then set it up to access to VMware Tanzu network to download tiles. Using a jumpbox is particularly useful in IaaSes where the <%= vars.ops_manager %> VM does not have a public IP address. In these cases, you can SSH into the <%= vars.ops_manager %> VM or any other component through the jumpbox.
    </td>
    <td>1</td>
  </tr>
</table>
