---
breadcrumb: Tanzu Operations Manager Documentation
title: Reference architectures for Tanzu Operations Manager and runtimes
owner: Customer0
---

This topic describes the reference architectures that you can use when installing <%= vars.ops_manager_first %> on any infrastructure to support the <%= vars.app_runtime_first %> and <%= vars.k8s_runtime_first %> runtime environments. These reference architectures describe proven approaches for deploying <%= vars.ops_manager %> and Runtimes, such as <%= vars.app_runtime_abbr %> or <%= vars.k8s_runtime_abbr %>, on a specific IaaS, such as AWS, Azure, GCP, OpenStack, or vSphere.

These reference architectures meet the following requirements:

* They are secure.
* They are publicly accessible.
* They include common <%= vars.ops_manager %>-managed services such as VMware Tanzu SQL, VMware Tanzu RabbitMQ, and Spring Cloud Services for VMware Tanzu.
* They can host at least 100 app instances.
* They have been deployed and validated by VMware to support <%= vars.ops_manager %>, <%= vars.app_runtime_abbr %>, and <%= vars.k8s_runtime_abbr %>.

You can use <%= vars.ops_manager %> reference architectures to help plan the best configuration for deploying your <%= vars.ops_manager %> on your IaaS.

## <a id='topics'></a> Reference architecture and planning topics

All <%= vars.ops_manager %> reference architectures start with the base <%= vars.app_runtime_abbr %> architecture and the base <%= vars.k8s_runtime_abbr %> architecture.

These IaaS-specific topics build on these two common base architectures:

* [AWS reference architecture for Tanzu Operations Manager](aws/aws_ref_arch.html)

* [Azure reference architecture for Tanzu Operations Manager](azure/azure_ref_arch.html)

* [GCP reference architecture for Tanzu Operations Manager](gcp/gcp_ref_arch.html)

* [OpenStack reference architecture for Tanzu Operations Manager](openstack/openstack_ref_arch.html)

* [vSphere reference architecture for Tanzu Operations Manager](vsphere/vsphere_ref_arch.html)

The following topics address aspects of platform architecture and planning that the <%= vars.ops_manager %>
reference architectures do not cover:

* [Implementing a multi-foundation <%= vars.k8s_runtime_abbr %> deployment](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid-Integrated-Edition/1.15/tkgi/GUID-nsxt-multi.html)
* [Global DNS load balancers for multi-foundation environments](global-dns-lb.html)

## <a id='pas-base'></a> <%= vars.app_runtime_abbr %> architecture

The following diagram illustrates a base architecture for <%= vars.app_runtime_abbr %> and how its network topology places and replicates <%= vars.app_runtime_abbr %> components across subnets and availability zones (AZs).

![TAS for VMs Base Topology. Each AZ includes Infrastructure subnet, PAS Subnet, Services subnet, On-demand services subnet, Isolation Segment subnet.](images/v2/export/PAS_Base.png)

[View a larger version of this diagram.](images/v2/export/PAS_Base.png)

### <a id='pas-components'></a> Internal components

The following table describes the internal component placements shown in the preceding diagram:

| Component | Placement and access notes |
| ------------- | ------------------------------ |
| <%= vars.ops_manager %> VM | Deployed on one of the three public subnets. Accessible by fully-qualified domain name (FQDN) or through an optional jumpbox. |
| BOSH Director | Deployed on the infrastructure subnet. |
| Jumpbox | Optional. Deployed on the infrastructure subnet for accessing <%= vars.app_runtime_abbr %> management components such as <%= vars.ops_manager %> and the Cloud Foundry Command Line Interface (cf CLI). |
| GoRouters (HTTP routers in <%= vars.ops_manager %>) | Deployed on all three <%= vars.app_runtime_abbr %> subnets, one per AZ. Accessed through the HTTP, HTTPS, and SSL load balancers. |
| Diego Brain | Deployed on all three <%= vars.app_runtime_abbr %> subnets, one per AZ. The Diego Brain component is required, but SSH container access support through the Diego Brain is optional, and enabled through the SSH load balancers. |
| TCP routers | Optional. Deployed on all three <%= vars.app_runtime_abbr %> subnets, one per AZ, to support TCP routing. |
| Service tiles | Service brokers and shared services instances are deployed to the Services subnet. Dedicated on-demand service instances are deployed to an on-demand services subnet. |
| Isolation segments | Deployed on an isolation segment subnet. Includes Diego Cells and GoRouters for running and accessing apps hosted within isolation segments. |

### <a id='pas-network'></a> Networks

The following sections describe <%= vars.app_runtime_abbr %>'s recommendations for defining your networks and load-balancing their incoming requests:

#### <a id='pas-subnets'></a> Required subnets

<%= vars.app_runtime_abbr %> requires these statically-defined networks to host its main component systems:

* Infrastructure subnet - `/24` segment<br>This subnet contains VMs that require access only for Platform Administrators, such as the <%= vars.ops_manager %> VM, the BOSH Director VM, and an optional jumpbox.

* <%= vars.app_runtime_abbr %> subnet - `/24` segment<br>This subnet contains <%= vars.app_runtime_abbr %> runtime VMs, such as Gorouters, Diego Cells, and Cloud Controllers.

* Services subnet - `/24` segment<br>The services and on-demand services networks support <%= vars.ops_manager %> tiles that you might add in addition to <%= vars.app_runtime_abbr %>. These are the networks for everything that is not <%= vars.app_runtime_abbr %>. Some services tiles can call for additional network capacity to grow into on-demand. If you use services with this capability, <%= vars.company_name %> recommends that you add an on-demand services network for each on-demand service.

* On-demand services subnets - `/24` segments<br>This is for services that can allocate network capacity on-demand from BOSH for their worker VMs. <%= vars.company_name %> recommends allocating a dedicated subnet to each on-demand service. For example, you can configure the Redis for VMware Tanzu tile as follows:
  * **Network:** Enter the existing `Services` network to host the service broker.
  * **Services network:** Deploy a new network `OD-Services1` to host the Redis worker VMs.
  <br>
  Another on-demand service tile can then also use `Services` for its broker and a new `OD-Services2` network for its workers, and so on.

* Isolation segment subnets - `/24` segments<br>You can add one or more isolation segment tiles to a <%= vars.app_runtime_abbr %> installation to compartmentalize hosting and routing resources. For each isolation segment you deploy, you can designate a `/24` network for its range of address space.

#### <a id='pas-lb'></a> Load balancing

Any <%= vars.app_runtime_abbr %> installation needs a suitable load balancer to send incoming HTTP, HTTPS, SSH, and SSL traffic to its GoRouters and app containers. All installations approaching production-level use rely on external load balancing from hardware appliance vendors or other network-layer solutions.

The load balancer can also perform Layer 4 or Layer 7 load balancing functions. SSL can be terminated at the load balancer or used as a pass-through to the Gorouter.

Common deployments of load balancing in <%= vars.app_runtime_abbr %> are:

* HTTP/HTTPS traffic to and from GoRouters

* TCP traffic to and from TCP routers

* Traffic from the Diego Brain, when developers access app containers through SSH

To load balance across multiple <%= vars.app_runtime_abbr %> foundations, use an IaaS-specific or vendor-specific Global Traffic Manager or Global DNS load balancer.

For more information, see [Global DNS load balancers for multi-foundation environments](global-dns-lb.html).

### <a id='pas-ha'></a> High availability

<%= vars.app_runtime_abbr %> is not considered high availability (HA) until it runs across at least two AZs. <%= vars.company_name %> recommends defining three AZs.

On an IaaS with its own HA capabilities, using the IaaS HA with a <%= vars.app_runtime_abbr %> HA
topology provides two benefits. Multiple AZs give <%= vars.app_runtime_abbr %> redundancy, so that losing an AZ is not
catastrophic. The BOSH Resurrector can then replace lost VMs as needed to repair a foundation.

To back up and restore a foundation externally, use BOSH backup and restore (BBR). For more information,
see the [BOSH backup and restore](https://docs.cloudfoundry.org/bbr/) documentation.

### <a id='pas-storage'></a> Storage

<%= vars.app_runtime_abbr %> requires disk storage for each component, both for persistent data and to allocate to ephemeral data. You size these disks in the **Resource Config** pane of the <%= vars.app_runtime_abbr %> tile. For more information about storage configuration and capacity planning, see the corresponding section in the [reference architecture for your IaaS](#topics).

The platform also requires you to configure file storage for large shared objects. These blobstores can be external or internal. For details, see [Configuring File Storage for <%= vars.app_runtime_abbr %>](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/<%= vars.dot_major_tas_version %>/tas-for-vms/pas-file-storage.html).

### <a id='pas-security'></a> Security

For information about how <%= vars.app_runtime_abbr %> implements security, see:

* [Tanzu Operations Manager security infrastructure](../security/pcf-infrastructure/index.html)

* [Security concepts in Tanzu Operations Manager](../security/concepts/index.html)

* [Certificates and TLS in your Tanzu Operations Manager deployment](../security/pcf-infrastructure/certificates-index.html)

* [Routing Network Communications](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/<%= vars.dot_major_tas_version %>/tas-for-vms/networking-routing-network-paths.html)

### <a id='pas-domains'></a> Domain names

<%= vars.app_runtime_abbr %> requires these domain names to be registered:

* System domain for <%= vars.app_runtime_abbr %> and other tiles: `sys.domain.name`
* App domain for your apps: `app.domain.name`

You must also define these wildcard domain names and include them when creating certificates that access <%= vars.app_runtime_abbr %>  and its hosted apps:

* `\*.SYSTEM-DOMAIN`

* `\*.APPS-DOMAIN`

* `\*.login.SYSTEM-DOMAIN`

* `\*.uaa.SYSTEM-DOMAIN`

### <a id='pas-scaling'></a> Component scaling

For recommendations about scaling <%= vars.app_runtime_abbr %> for different deployment scenarios, see [Scaling <%= vars.app_runtime_abbr %> ](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/<%= vars.dot_major_tas_version %>/tas-for-vms/scaling-ert-components.html).

## <a id='pks-base'></a> <%= vars.k8s_runtime_abbr %> Architecture

The following diagram illustrates a base architecture for <%= vars.k8s_runtime_abbr %> and how its network topology places and replicates <%= vars.k8s_runtime_abbr %> components across subnets and AZs:

![TKGI Base Deployment Topology. Each AZ includes Infrastructure subnet, Services subnet, and PKS Cluster Subnets, which are dynamically generated.](images/v2/export/PKS_Base.png)

[View a larger version of this diagram.](images/v2/export/PKS_Base.png)

### <a id='pks-components'></a> Internal components

The following table describes the internal component placements shown in the preceding diagram:

| Component | Placement and Access Notes |
| --------- | -------------------------- |
| <%= vars.ops_manager %> VM | Deployed on one of the subnets. Accessible by fully-qualified domain name (FQDN) or through an optional jumpbox. |
| BOSH Director | Deployed on the infrastructure subnet. |
| Jumpbox | Optional. Deployed on the infrastructure subnet for accessing <%= vars.k8s_runtime_abbr %> management components such as <%= vars.ops_manager %> and the `kubectl` command line interface. |
| <%= vars.k8s_runtime_abbr %> API | Deployed as a service broker VM on the <%= vars.k8s_runtime_abbr %> services subnet. Handles <%= vars.k8s_runtime_abbr %> API and service adapter requests, and manages <%= vars.k8s_runtime_abbr %> clusters. For more information, see [<%= vars.k8s_runtime_abbr %> Architecture](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid-Integrated-Edition/1.16/tkgi/GUID-control-plane.html). |
| Harbor tile | Optional container images registry, typically deployed to the services subnet. |
| <%= vars.k8s_runtime_abbr %> Cluster | Deployed to a dynamically-created, dedicated <%= vars.k8s_runtime_abbr %> cluster subnet. Each cluster consists of worker nodes that run the workloads, or apps, and one or more primary nodes. |

### <a id='pks-network'></a> Networks

These sections describe <%= vars.company_name %>'s recommendations for defining your networks and load-balancing their incoming requests.

#### <a id='pks-subnets'></a> Subnets requirements

<%= vars.k8s_runtime_abbr %> requires two defined networks to host the main elements:

* Infrastructure subnet - `/24`<br>This subnet contains VMs that require access only for Platform Administrators, such as the <%= vars.ops_manager %> VM, the BOSH Director VM, and an optional jumpbox.

* <%= vars.k8s_runtime_abbr %> services subnet - `/24`<br>This subnet hosts <%= vars.k8s_runtime_abbr %> API VM and other optional service tiles such as VMware Harbor Registry.

* <%= vars.k8s_runtime_abbr %> cluster subnets - each one a `/24` from a pool of pre-allocated IPs<br>These subnets host <%= vars.k8s_runtime_abbr %> clusters.

#### <a id='pks-lb'></a> Load balancing

Load balancers can be used to manage traffic across primary nodes of a <%= vars.k8s_runtime_abbr %> cluster or for deployed workloads. For more information on how to configure load balancers for <%= vars.k8s_runtime_abbr %>, see the corresponding section in the [reference architecture for your IaaS](#topics).

### <a id='pks-ha'></a> High availability

<%= vars.k8s_runtime_abbr %> has no inherent HA capabilities to design for. Make the best efforts to have HA design at the IaaS, storage, power, and access layers to support <%= vars.k8s_runtime_abbr %>.

### <a id='pks-storage'></a> Storage

<%= vars.k8s_runtime_abbr %> requires shared storage across all AZs for the deployed workloads to appropriately allocate their required storage.

### <a id='pks-security'></a> Security

For information about how <%= vars.k8s_runtime_abbr %> implements security, see [Enterprise <%= vars.k8s_runtime_abbr %> Security](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid-Integrated-Edition/1.16/tkgi/GUID-security.html) and [Firewall Ports](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid-Integrated-Edition/1.16/tkgi/GUID-ports-protocols-nsx-t.html).

### <a id='pks-domains'></a> Domain names

<%= vars.k8s_runtime_abbr %> requires the `*.pks.domain.name` domain name to be registered when creating a wildcard certificate and <%= vars.k8s_runtime_abbr %> tile configurations.

The wildcard certificate covers both the <%= vars.k8s_runtime_abbr %> API domain, such as `api.pks.domain.name`, and the <%= vars.k8s_runtime_abbr %> clusters domains, such as `cluster.pks.domain.name`.

### <a id='pks-management'></a> Cluster management

For information about managing <%= vars.k8s_runtime_abbr %> clusters, see [Managing clusters](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid-Integrated-Edition/1.16/tkgi/GUID-managing-clusters.html).
