---
title: vSphere Reference Architecture
owner: Customer0
---

This topic describes a reference architecture for Ops Manager and any runtime products, including Pivotal Application Service (PAS) and VMware Tanzu Kubernetes Grid Integrated Edition (TKGI), on vSphere.
It builds on the common base architectures described in [Platform Architecture and Planning](../index.html).

For additional requirements and installation instructions for Ops Manager on vSphere, see [Installing Ops Manager on vSphere](../../install/vsphere.html).


## <a id="overview"></a> Before You Begin

Ops Manager products contain feature sets that, when used, impact designs in this reference architecture.

### <a id="ist"></a> Isolation Segments

Isolation segments are delivered as a tile in Ops Manager. They are used to create smaller individual deployments of Pivotal Application Service (PAS) components, Gorouters, and Diego Cells to help isolate workloads. For example, the isolation criteria could include capacity management or audit compliance. Isolation segments can be also be patched independently from the PAS tile or other isolation segments.

<%= vars.recommended_by %> recommends that when using isolation segments, you should deploy all containers to an isolation segment, and deploy only system-level components to the PAS tile. By moving all app workloads to an isolation segment, you get more control over the network usage, capacity planning, firewalling, and business continuity.

### <a id="host-groups"></a> Host Groups

Host groups allow you create virtual PaaS failure domains independently of physical IaaS capacity. In PAS or VMware Tanzu Kubernetes Grid Integrated Edition (TKGI) on vSphere, this amounts to creating multiple Availability Zones (AZs) that do not have to align perfectly with vSphere constructs, such as clusters or pools.

You can use host groups to gain PaaS high availability (HA) with undersized IaaS capacity. Alternatively, host groups can be used to create AZs that can grow in an elastic way as the IaaS grows.

### <a id="multi-vcenter"></a> Multiple vCenter Support

Multiple vCenter Support can allow you to use PAS in a mature vSphere environment. Coupled with host groups, a brown field deployment strategy can happen as hosts in existing vSphere clusters can be set aside for PAS or TKGI. You can also structure PAS or TKGI geographically without needing to stretch IaaS elements.


## <a id="overview"></a> Overview

### <a id="pas-nsx-t-ha"></a> High Availability

For information about HA requirements and recommendations for PAS on vSphere, see [High Availability](../index.html#pas-ha) in _Platform Architecture and Planning Overview_.

### <a id="pas-nsx-t-shared-storage"></a> Shared Storage

Ops Manager requires shared storage. You can allocate networked storage to the host clusters following one of two common approaches: horizontal or vertical. The approach you follow reflects how your data center arranges its storage and host blocks in its physical layout.

#### <a id="horizontal-shared-storage"></a> Horizontal Shared Storage

With the horizontal shared storage approach, you grant all hosts access to all datastores and assign a subset to each Ops Manager installation.

For example, with six datastores `ds01` through `ds06`, you grant all nine hosts access to all six datastores. You then provision your first Ops Manager installation to use stores `ds01` through `ds03`  and your second Ops Manager installation to use `ds04` through `ds06`.

#### <a id="vertical-shared-storage"></a> Vertical Shared Storage

With the vertical shared storage approach, you grant each cluster its own datastores, creating a cluster-aligned storage strategy. vSphere VSAN is an example of this architecture.

For example, with six datastores `ds01` through `ds06`, you assign datastores `ds01` and `ds02` to a cluster, `ds03` and `ds04` to a second cluster, and `ds05` and `ds06` to a third cluster. You then provision your first Ops Manager installation to use `ds01`, `ds03`, and `ds05`, and your second Ops Manager installation to use `ds02`, `ds04`, and `ds06`.

With this arrangement, all VMs in the same installation and cluster share a dedicated datastore.

### <a id="pas-nsx-t-storage-capacity"></a> Storage Capacity

<%= vars.recommended_by %> recommends these storage capacity allocations for production and non-production PAS environments:

* **Production environments**: Configure at least 8&nbsp;TB of data storage. You can configure this as either one 8&nbsp;TB store or a number of smaller volumes that sum to 8&nbsp;TB. Frequently-used developments may require significantly more storage to accommodate new code and buildpacks.

* **Non-production environments**: Configure 4 to 6&nbsp;TB of data storage.

<p class="note"><strong>Note:</strong> Ops Manager does not support using vSphere Storage Clusters with the latest versions of Ops Manager validated for the reference architecture. Datastores should be listed in the vSphere tile by their native name, not the cluster name created by vCenter for the storage cluster.</p>

<p class="note"><strong>Note:</strong> If a datastore is part of a vSphere Storage Cluster using DRS storage (sDRS), you must disable the s-vMotion feature on any datastores used by Ops Manager. Otherwise, s-vMotion activity can rename independent disks and cause BOSH to malfunction. For more information, see <a href="./vsphere_migrate_datastore.html">Migrating Pivotal Platform to a New Datastore in vSphere</a>.</p>

For more information about general storage requirements and recommendations for PAS, see [Storage](../index.html#pas-storage) in _Platform Architecture and Planning Overview_.

### <a id="pas-nsx-t-sql"></a> SQL Server

An internal MySQL database is sufficient for use in production environments.

However, an external database provides more control over database management for large environments that require multiple data centers.

For information about configuring system databases on PAS, see [Configure System Databases](https://docs.pivotal.io/application-service/2-7/operating/configure-pas.html#sys-db) in _Configuring PAS_.

### <a id="pas-nsx-t-security"></a> Security

For information about security requirements and recommendations for PAS deployments, see [Security](../index.html#pas-security) in _Platform Architecture and Planning Overview_.

### <a id="pas-nsx-t-blobstore"></a> Blobstore Storage

PAS ships with an internal blobstore. It is recommended for POC deployments. <%= vars.recommended_by %> recommends that you use these blobstore storages for production and non-production PAS environments:

* **Production/test environments:** Use an external S3 storage appliance as the blobstore, such as Dell ECS, Minio, or any other S3-compatible datastore in your environment.

* **POC environments:** Use the internal blobstore.

<p class="note"><strong>Note:</strong> For POC environments, the internal blobstore can be the primary consumer of storage, as the internal blobstore must be actively maintained. PAS deployments experience downtime during events such as storage upgrades or migrations to new disks.</p>

#### <a id="pas-nsx-t-firewall"></a> Firewall

<%= partial '../../common/vsphere-firewall-ports' %>

#### <a id="pas-nsx-t-dns"></a> DNS

PAS requires a system domain, app domain, and several wildcard domains.

For more information about DNS requirements for PAS, see [Domain Names](../index.html#pas-domains) in _Platform Planning and Architecture_.

### <a id="networking"></a> Networking

The vSphere reference architecture for the PAS and TKGI runtimes is based on software-defined networking (SDN) infrastructure. vSphere offers NSX-T and NSX-V to support SDN infrastructure.

<%= vars.recommended_by %> recommends using an SDN to take advantage of features including:

* Virtualized, encapsulated networks and encapsulated broadcast domains

* VLAN exhaustion avoidance with the use of virtualized logical networks

* DNAT/SNAT services to create separate, non-routable network spaces for the PAS installation

* Load balancing services to pass traffic through Layer 4 to pools of platform routers at Layer 7

* SSL termination at the load balancer at Layer 7 with the option to forward on at Layer 4 or 7 with unique certificates

* Virtual, distributed routing and firewall services native to the hypervisor

Ops Manager supports these configurations for vSphere deployments:

* PAS on vSphere with NSX-T. For more information, see [PAS on vSphere with NSX-T](#pas-nsx-t).

* PAS on vSphere with NSX-V. For more information, see [PAS on vSphere with NSX-V](#pas-nsx-v).

* PAS on vSphere without NSX. For more information, see [PAS on vSphere without NSX](#pas-routed).

* TKGI on vSphere with NSX-T. For more information, see [TKGI on vSphere with NSX-T](#pks-nsx-t).

* TKGI on vSphere without NSX-T. For more information, see [TKGI on vSphere without NSX-T](#pks-without-nsx-t).


## <a id="pas-nsx-t"></a> PAS on vSphere with NSX-T

These sections describe the reference architecture for PAS on vSphere with NSX-T deployments. They also provide requirements and recommendations for deploying PAS on vSphere with NSX-T, such as network, load balancing, and storage capacity requirements and recommendations.

### <a id="pas-nsx-t-diagram"></a> Architecture

The diagram below illustrates reference architecture for PAS on vSphere with NSX-T deployments:

![The diagram shows the architecture for a PAS on vSphere with NSX-T deployment. For more information about the components and networking demonstrated by the diagram, read the description below this diagram.](../images/v2/export/PAS_vSphere_NSX-T.png)

PAS deployments with NSX-T are deployed with three clusters and three AZs.

An NSX-T Tier-0 router is on the front end of the PAS deployment. This router is a central logical router into and out of the PAS platform. Routing configuration on the IP backbone can be static or dynamic using BGP on the Tier-0 router. Several Tier-1 routers, such as the router for the PAS and infrastructure subnets, connect to the Tier-0 router as child routing points to and from other child T1 routers and from the routed IP backbone.

### <a id="pas-nsx-t-diagram"></a> NSX-T Container Plugin Requirement

PAS deployments require the VMware NSX-T Container Plugin for Ops Manager to enable the SDN features available through NSX-T.

The NSX-T Container Plugin enables a container networking stack and integrates with NSX-T.

<p class="note"><strong>Note:</strong> To use NSX-T with PAS, the NSX-T Container Plugin must be installed, configured, and deployed at the same time as the PAS tile. To download the NSX-T Container Plugin, go to the <a href="https://network.pivotal.io/products/vmware-nsx-t">VMware NSX-T Container Plug-in</a> page on Pivotal Network.</p>

### <a id="pas-nsx-t-networking"></a> Networking

These sections describe networking requirements and recommendations for PAS on vSphere with NSX-V deployments.

#### <a id="pas-nsx-t-routable-ip"></a> Routable IPs

The Tier-0 router must have routable external IP address space assigned to peer with other routers on that backbone (BGP routing) or simply be an addressable routing point (static routing). Select a network range for the Tier-0 router with enough addresses, as the network is separated into these two jobs:

* Routing incoming and outgoing traffic on the IP backbone.
* DNATs and SNATs, load balancer VIPs, and other Ops Manager components.

<p class="note"><strong>Note:</strong> Compared to NSX-V, NSX-T consumes much more address space for SNATs.</p>

#### <a id="pas-nsx-t-load-balancing"></a> Load Balancing

The load balancing requirements and recommendations for PAS on vSphere with NSX-T deployments are:

* You must configure NSX-T load balancers for the Gorouters.
  * The domains for the PAS system and apps must resolve to the load balancer VIP.
  * You must assign either a private or a public IP address assigned to the domains for the PAS system and apps.

* <%= vars.recommended_by %> recommends that you configure load balancers at Layer 4 for the Gorouters. With Layer 4 load balancers, HTTP/HTTPS traffic passes through the load balancers and SSL is terminated at the Gorouters. This approach reduces overhead processing. This burden is better shared amongst a number of Gorouters as compared to a single, logical NSX load balancer.
  <p class='note'><strong>Note:</strong> It is possible to use Layer 7 load balancers and terminate or initiate SSL at the load balancers. However, <%= vars.recommended_by %> does not recommend this approach, since it adds additional overhead processing.</p>

* Any TCP Gorouters and SSH Proxies within the platform also require NSX-T load balancers.

* Layer 4 and Layer 7 NSX-T load balancers are created automatically during app deployment.

#### <a id="pas-nsx-t-networking-subnets-ip"></a> Networking, Subnets, and IP Spacing

The requirements and recommendations related to networks, subnets, and IP spacing for PAS on vSphere with NSX-T deployments are:

* PAS requires statically-defined networks to host PAS component VMs.

* The tenant side uses a series of non-routable address ranges when using NAT.

* NSX-T dynamically assigns PAS org networks and adds a Tier-1 router. These org networks are automatically instantiated based on a non-overlapping block of address space. You can configure the block of address space in the **NCP Configuration** section of the NSX-T tile in Ops Manager. The default is `/24`. This means that every org in PAS is assigned a new `/24` network.

For more information about PAS subnets, see [Required Subnets](../index.html#pas-network) in _Platform Architecture and Planning Overview_.

Below is an example subnet layout using NAT:

* Infrastructure - 192.168.1.0/24
* Deployment - 192.168.2.0/24
* Services - 192.168.3.0/24
* On-demand service - 192.168.4.0 - 192.168.9.255 (in /24 segments)
* Isolation segments - 192.168.10.0 - 192.168.127.255 (in /24 segments)
* On-demand orgs - 192.168.128.0/17 (in /24 segments - auto-allocated by NSX-T only)


## <a id="pas-nsx-v"></a> PAS on vSphere with NSX-V

These sections describe the reference architecture for PAS on vSphere with NSX-V deployments. They also provide requirements and recommendations for deploying PAS on vSphere with NSX-V, such as network, load balancing, and storage capacity requirements and recommendations.

PAS on vSphere with NSX-V enables services provided by NSX on the PAS platform, such as an Edge services gateway (ESG), load balancers, firewall services, and NAT/SNAT services.

### <a id="pas-nsx-v-diagram"></a> Architecture

The diagram below illustrates the reference architecture for PAS on vSphere with NSX-V deployments.

![The diagram shows the architecture for a PAS on vSphere with NSX-V deployment. For more information about the components and networking demonstrated by the diagram, read the description below this diagram.](../images/v2/export/PAS_vSphere_NSX-V.png)

[View a larger version of this diagram](../images/v2/export/PAS_vSphere_NSX-V.png).

PAS deployments with NSX-V are deployed with three clusters and three AZs.

PAS deployments with NSX-V also include an NSX-V Edge router on the front end.

North-south traffic in NSX-V is handled by the Edge Services Gateway (ESG). Traffic entering and leaving a PAS installation as a tenant behind an ESG is considered north-south. Traffic between the networks deployed on the tenant side of an ESG is considered east-west, which could be routed by the ESG or by a distributed logical router (DLR).

Compared to NSX-T architecture, NSX-V architecture does not use Tier-1 routers to connect the central router to the various subnets for the PAS deployment.

For more information about using an ESG on vSphere, see [Using Edge Services Gateway on VMware NSX](vsphere_nsx_cookbook.html).

### <a id="pas-nsx-v-networking"></a> Networking

These sections describe networking requirements and recommendations for PAS on vSphere with NSX-V deployments.

#### <a id="pas-nsx-v-routable-ip"></a> Routable IPs

You must assign routable external IPs on the server side, such as routable IPs for NATs and load balancers, to the Edge router.

#### <a id="pas-nsx-v-firewall"></a> Firewall

<%= partial '../../common/vsphere-firewall-ports' %>

#### <a id="pas-nsx-v-dns"></a> DNS

Load balancing services are available in the ESG. These services are processes that run on the ESG along with other services, such as routing and the firewall.

The load balancing requirements and recommendations for PAS on vSphere with NSX-V deployments are:

* ESG load balancers can be configured as either Layer 4 (encryption pass-through) or Layer 7 (encryption termination and initiation). <%= vars.recommended_by %> recommends using Layer 4 services at the ESG in order to reduce processing overhead in the ESG.
  <p class='note'><strong>Note:</strong> It is possible to use Layer 7 load balancers and terminate SSL at the load balancers. However, <%= vars.recommended_by %> does not recommend this approach, since it adds additional overhead processing.</p>

* The domains for the PAS system and apps must resolve to the load balancer. If the ESG is also the load balancer, these domains will resolve to a VIP on that ESG.

* If you are also using features such as TCP routing or a proxy for SSH, you can deploy a load balancer service for them in the same, or a different ESG.

* If you do not want to use load balancing services in NSX-V, or use load balancing services external to NSX-V in addition to those included, they should be deployed external to (on the provider side of) the NSX-V installation.

A high-performance alternative to load balancing in the ESG that is also the boundary router is to deploy ESGs as dedicated load balancer service nodes on a separate network on the tenant side of the routing ESG.

These specific ESG load balancers act as "one-arm load balancers", where they are connected to the separate network with a single interface and act as both a load balancer VIP and the owner of the pool being load-balanced to throughout that connection. Multiples of these can be deployed to ensure compute resources are dedicated to these load balancing needs with no overhead from any other job.

The key difference from the typical approach is that the VIP and pool are both aligned to a single network connection, whereas the typical approach is having the VIP on the provider interface and the pool on the tenant interface.

#### <a id="pas-nsx-v-ip-spacing"></a> Networks, Subnets, and IP Spacing

For information about network, subnet, and IP space planning requirements and recommendations, see [Required Subnets](../index.html#pas-network) in _Platform Architecture and Planning Overview_.

### <a id="pas-nsx-v-ha"></a> High Availability

For information about HA requirements and recommendations for PAS on vSphere, see [High Availability](../index.html#pas-ha) in _Platform Architecture and Planning Overview_.

### <a id="pas-nsx-v-shared-storage"></a> Shared Storage

Ops Manager requires shared storage. You can allocate networked storage to the host clusters following one of two common approaches: horizontal or vertical. The approach you follow reflects how your data center arranges its storage and host blocks in its physical layout.

For information about horizontal and vertical shared storage, see [Shared Storage](#pas-nsx-t-shared-storage).

### <a id="pas-nsx-v-storage-capacity"></a> Storage Capacity

<%= vars.recommended_by %> recommends these storage capacity allocations for production and non-production PAS environments:

* **Production environments**: Configure at least 8&nbsp;TB of data storage. You can configure this as either one 8&nbsp;TB store or a number of smaller volumes that sum to 8&nbsp;TB. Frequently-used developments may require significantly more storage to accommodate new code and buildpacks.

* **Non-production environments**: Configure 4 to 6&nbsp;TB of data storage.

<p class="note"><strong>Note:</strong> The latest versions of Ops Manager validated for the reference architecture do not support using vSphere Storage Clusters. Datastores should be listed in the vSphere tile by their native name, not the cluster name created by vCenter for the storage cluster.</p>

<p class="note"><strong>Note:</strong> If a datastore is part of a vSphere Storage Cluster using DRS storage (sDRS), you must disable the s-vMotion feature on any datastores used by Ops Manager. Otherwise, s-vMotion activity can rename independent disks and cause BOSH to malfunction. For more information, see <a href="./vsphere_migrate_datastore.html">How to Migrate Ops Manager to a New Datastore in vSphere</a>.</p>

For more information about general storage requirements and recommendations for PAS, see
[Storage](../index.html#pas-storage) in _Platform Architecture and Planning Overview_.

### <a id="pas-nsx-v-sql"></a> SQL Server

An internal MySQL database is sufficient for use in production environments.

However, an external database provides more control over database management for large environments that require multiple data centers.

For information about configuring system databases on PAS, see [Configure System Databases](https://docs.pivotal.io/application-service/2-7/operating/configure-pas.html#sys-db) in _Configuring PAS_.

### <a id="pas-nsx-v-security"></a> Security

For information about security requirements and recommendations for PAS on vSphere deployments, see [Security](../index.html#pas-security) in _Platform Architecture and Planning Overview_.

### <a id="pas-nsx-v-blobstore"></a> Blobstore Storage

PAS ships with an internal blobstore. It is recommended for POC deployments. <%= vars.recommended_by %> recommends that you use these blobstore storages for production and non-production PAS environments:

* **Production and test environments:** Use an external S3 storage appliance as the blobstore, such as Dell ECS, Minio, or any other S3-compatible datastore in your environment.

* **Non-production environments:** Use the internal blobstore.

<p class="note"><strong>Note:</strong> For POC environments, the internal blobstore can be the primary consumer of storage, as the internal blobstore must be actively maintained. PAS deployments experience downtime during events such as storage upgrades or migrations to new disks.</p>

For more information about blobstore storage requirements and recommendations, see [Configure File Storage](https://docs.pivotal.io/application-service/<%= vars.current_major_version %>/operating/configuring.html#file-storage) in _Configuring PAS for Upgrades_.


## <a id="pas-routed"></a> PAS on vSphere without NSX

These sections describe the architecture for PAS on vSphere without software-defined networking deployments.

<p class="note"><strong>Note:</strong> This architecture was validated for earlier versions of PAS.
However, it has not been validated for PAS v2.7.
</p>

### <a id="pas-routed-networking"></a> Networking

Without an SDN, IP allocations all come from routed network space. Discussions and planning within your organization are essential to acquiring the necessary amount of IP space for a PAS deployment with future growth considerations. This is because routed IP address space is a premium resource, and adding more later is difficult, costly, and time-consuming.

Below is a best-guess layout for IP space utilization in a single PAS deployment:

* Infrastructure - /28

* PAS deployment - /23<br>This size is almost completely dependent on the estimated desired capacity for containers. It can be smaller, but <%= vars.recommended_by %> does not recommend using a larger size in a single deployment.

* Services - /23<br>This size is almost completely dependent on the estimated desired capacity for services. Resize as necessary.

### <a id="pas-routed-iso-seg"></a> Isolation Segments

Isolation segments can help with satisfying IP address space needs in a routed network design. You can build smaller groups of Gorouters and Diego Cells aligned to a particular service. Smaller groups use less IP address space.


## <a id="pks-nsx-t"></a> TKGI on vSphere with NSX-T

These sections describe the reference architecture for TKGI on vSphere with NSX-T deployments. They also provide requirements and recommendations for deploying TKGI on vSphere with NSX-T, such as network, load balancing, and storage capacity requirements and recommendations.

### <a id="pks-nsx-t-diagram"></a> Architecture

The diagram below illustrates the reference architecture for TKGI on vSphere with NSX-T deployments.

![The diagram shows the architecture for a TKGI on vSphere with NSX-T deployment. For more information about the components and networking demonstrated by the diagram, read the description below this diagram.](../images/v2/export/PKS_vSphere_NSX-T.png)

[View a larger version of this diagram](../images/v2/export/PKS_vSphere_NSX-T.png).

TKGI deployments with NSX-T are deployed with three clusters and three AZs.

An NSX-T Tier-0 router is on the front end of the TKGI deployment. This router is a central logical router into the TKGI platform. You can configure static or dynamic routing using BGP from the routed IP backbone through the Tier-0 router.

Several Tier-1 routers, such as the router for the infrastructure subnet, connect to the Tier-0 router. New Tier-1 routers are created on-demand as new clusters and namespaces are added to TKGI.

<p class="note"><strong>Note:</strong> The TKGI on vSphere with NSX-T architecture supports multiple master nodes for TKGI v1.2 and later.</p>

### <a id="pks-nsx-t-networking"></a> Networking

These sections describe networking requirements and recommendations for TKGI on vSphere with NSX-T deployments.

#### <a id="pks-nsx-t-load-balancing"></a> Load Balancing

The load balancing requirements and recommendations for TKGI on vSphere with NSX-T deployments are:

* Use standard NSX-T load balancers. Layer 4 and Layer 7 NSX-T load balancers are created automatically during app deployment.

* Use both Layer 4 and Layer 7 load balancers:
	* Use Layer 7 load balancers for ingress routing.
	* Use Layer 4 load balancers for `LoadBalancer` services. This allows you to terminate SSL at the load balancers, which reduces overhead processing.

* NSX-T provides ingress routing natively. You can also use a third-party service for ingress routing, such as Istio or Nginx. You run the third-party ingress routing service as a container in the cluster.

* If you use a third-party ingress routing service, you must:
	* Create wildcard DNS entries to point to the service.
	* Define domain information for the ingress routing service in the manifest of the TKGI on vSphere deployment. For example:

		```
		apiVersion: extensions/v1beta1
		kind: Ingress
		metadata:
		  name: music-ingress
		  namespace: music1
		spec:
		  rules:
		  - host: music1.pks.domain.com
		    http:
		      paths:
		      - path: /.*
		        backend:
		          serviceName: music-service
		          servicePort: 8080
		```

* When you push a TKGI on vSphere deployment with a service `type` set to `LoadBalancer`, NSX-T automatically creates a new WIP for the deployment on the existing load balancer for that namespace. You must specify a listening and translation port in the service, a name for tagging, and a protocol. For example:

	```
	apiVersion: v1
	kind: Service
	metadata:
	  ...
	spec:
	  type: LoadBalancer
	  ports:
	  - port: 80
	    targetPort: 8080
	    protocol: TCP
	    name: web
    ```

#### <a id="pks-nsx-t-external-routing"></a> Routable IPs

The routable IP requirements and recommendations for TKGI with NSX-T deployments are:

* **Deployments with TKGI NSX-T ingress:** <%= vars.recommended_by %> recommends a `/25` network for deployments with TKGI NSX-T ingress. The Tier-0 router must have routable external IP address space to advertise on the BGP network with its peers.
	</br>
	</br>
	Select a network range for the Tier-0 router with enough space so that the network can be separated into these two jobs:
    * Routing incoming and outgoing traffic
    * DNATs and SNATs, load balancer WIPs, and other Ops Manager components
	<p class="note"><strong>Note:</strong> Compared to vSphere deployments with NSX-V, TKGI on vSphere with NSX-T consumes much more address space for SNATs.</p>

* **Deployments with several load balancers:** <%= vars.recommended_by %> recommends a `/23` network for deployments that use several load balancers. Deployments with several load balancers have much higher address space consumption for load balancer WIPs. This is because Kubernetes service types allocate IP addresses very frequently. To accommodate the higher address space, allow for four times the address space.

#### <a id="pks-nsx-t-firewall"></a> Firewall

<%= partial '../../common/vsphere-firewall-ports' %>

#### <a id="pks-nsx-t-ip-spacing"></a> Networks, Subnets, and IP Address Spacing

These considerations and recommendations apply to networks, subnets, and IP spacing for TKGI on vSphere with NSX-T deployments:

* Allocate a large network block for TKGI clusters and pods:
	* **TKGI clusters:** Configure a `172.24.0.0/14` network block.
	* **TKGI pods:** Configure a `172.28.0.0/14` network block.
  <br>
  <br>
	NSX-T creates IP address blocks of `/24` from these `/14` networks by default each time a new cluster or pod is created. You can configure this CIDR range for TKGI in Ops Manager.

* When deploying TKGI with Ops Manager, you must allow for a block of address space for dynamic networks that TKGI deploys for each namespace. The recommended address space allows you to view a queue of which jobs relate to each service.

* When a new TKGI cluster is created, TKGI creates a new `/24` network from TKGI cluster address space.

* When a new app is deployed, new NSX-T Tier-1 routers are generated and TKGI creates a `/24` network from the TKGI pods network.

* Allocate a large IP block in NSX-T for Kubernetes pods. For example, a `/14` network. NSX-T creates address blocks of `/24` by default. This CIDR range for Kubernetes services network ranges is configurable in Ops Manager.

For more information, see [Networks](../index.html#pks-network) in _Platform Architecture and Planning Overview_.

### <a id="pks-nsx-t-multitenancy"></a> Multi-Tenancy

For TKGI on vSphere with NSX-T deployments, networks are created dynamically for both TKGI clusters and pods.

To accommodate these dynamically-created networks, <%= vars.recommended_by %> recommends that you use multiple clusters, rather than a single cluster with multiple namespaces.

Multiple clusters provide additional features such as security, customization on a per-cluster basis, privileged containers, failure domains, and version choice. Namespaces should be used as a naming construct and not as a tenancy construct.

### <a id="pks-nsx-t-master-nodes"></a> Master Nodes

The TKGI on vSphere with NSX-T architecture supports multiple master nodes for TKGI v1.2 and later.

You can define the number of master nodes per plan in the TKGI tile in Ops Manager. The number of master nodes should be an odd number to allow `etcd` to form a quorum.

<%= vars.recommended_by %> recommends that you have at least one master node per AZ for HA and disaster recovery.

### <a id="pks-nsx-t-ha"></a> High Availability

For information about HA requirements and recommendations, see [High Availability](../index.html#pks-ha) in _Platform Architecture and Planning Overview_.

### <a id="pks-nsx-t-storage-capacity"></a> Storage Capacity

<%= vars.recommended_by %> recommends the following storage capacity allocation for production and non-production TKGI environments:

TKGI on vSphere supports static persistent volume provisioning and dynamic persistent volume provisioning.

For more information about storage requirements and recommendations, see [PersistentVolume Storage Options on vSphere](https://docs.pivotal.io/pks/vsphere-persistent-storage.html)

### <a id="pks-nsx-t-security"></a> Security

For information about security requirements and recommendations, see [Security](../index.html#pks-security) in _Platform Architecture and Planning Overview_.

## <a id="pks-without-nsx-t"></a> TKGI on vSphere without NSX-T

You can deploy TKGI without NSX-T.
If you want to deploy TKGI without NSX-T, select
**Flannel** as your container network interface in the **Networking** pane of
the TKGI tile.

Select from networks already identified in Ops Manager to deploy the
TKGI API and TKGI-provisioned
Kubernetes clusters.
